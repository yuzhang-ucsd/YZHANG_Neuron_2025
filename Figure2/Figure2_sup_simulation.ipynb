{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from scipy.stats import sem\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "random_seed = 20250323\n",
    "random.seed(random_seed)\n",
    "np.random.seed(random_seed)\n",
    "\n",
    "# General Processing Functions\n",
    "def prepare_hist_matrix(a, R, n_back):\n",
    "    # Prepare history matrix\n",
    "    num_trials = len(a)\n",
    "    uR = np.zeros(num_trials)\n",
    "    uR[(R==0) & (a<3)] = 1\n",
    "\n",
    "    # Now pads with zeros\n",
    "    a_hist = np.zeros((num_trials, n_back))\n",
    "    R_hist = np.zeros((num_trials, n_back))\n",
    "    uR_hist = np.zeros((num_trials, n_back))\n",
    "\n",
    "    a_cleaned = a.copy()+0.\n",
    "    a_cleaned[a>2] = 0\n",
    "    a_cleaned[a==2] = -1\n",
    "\n",
    "    # fill history matrices with n_back information\n",
    "    for nn in range(n_back):\n",
    "        a_hist[(1+nn):,nn] = a_cleaned[0:num_trials-nn-1]\n",
    "        R_hist[(1+nn):,nn] = R[0:num_trials-nn-1]\n",
    "        uR_hist[(1+nn):,nn] = uR[0:num_trials-nn-1]\n",
    "\n",
    "    return np.fliplr(a_hist), np.fliplr(R_hist), np.fliplr(uR_hist)\n",
    "\n",
    "# Basic logistic Regression fitting functions\n",
    "def logistic_prepare_predictors(a, R, n_back):\n",
    "    num_trials = len(a)\n",
    "\n",
    "    # Get history matrices\n",
    "    a_hist, R_hist, _ = prepare_hist_matrix(a, R, n_back)\n",
    "\n",
    "    # create and align component arrays\n",
    "    past_r = np.multiply(a_hist, R_hist)\n",
    "    past_ur = np.multiply(a_hist, R_hist==0)\n",
    "    past_c = a_hist\n",
    "\n",
    "    # identify choice trials\n",
    "    choice_trials = (a==1)|(a==2)\n",
    "\n",
    "    left_trials = (a==1)+0\n",
    "    left_trials[a==2]=-1\n",
    "\n",
    "    left_choice_trials = left_trials[choice_trials]\n",
    "\n",
    "    # create output arrays: predictors and targets\n",
    "    glm_mat = np.concatenate((past_r[choice_trials,:], past_ur[choice_trials,:]), axis=1)\n",
    "    glm_target = left_choice_trials\n",
    "    return glm_mat, glm_target\n",
    "\n",
    "# Simulation loop\n",
    "def simulation_session_loop(environment_properties, agent_properties, num_trials):\n",
    "    # Initialize trial_history_dict\n",
    "    trial_history_dict = {'Trial': [],\n",
    "                         'l_prob': [],\n",
    "                         'r_prob': [],\n",
    "                         'lreward_available': [],\n",
    "                         'rreward_available': [],\n",
    "                         'Choice': [],\n",
    "                         'Reward':[],\n",
    "                         'p_left':[]}\n",
    "    for tt in range(1, num_trials+1):\n",
    "        trial_history_dict, lr = simulation_reward_assignment(trial_history_dict, tt, environment_properties)\n",
    "        trial_history_dict, choice = simulation_agent(trial_history_dict, tt, agent_properties)\n",
    "\n",
    "        # Decide choice\n",
    "        temp_random = random.random()\n",
    "        if temp_random < agent_properties['miss rate']:\n",
    "            choice = 4  # Choice is Miss\n",
    "        elif temp_random < (agent_properties['miss rate'] + agent_properties['alarm rate']):\n",
    "            choice = 3  # Choice is Alarm\n",
    "\n",
    "        trial_history_dict['Choice'][-1] = choice\n",
    "\n",
    "        # Evaluate Choice\n",
    "        if (choice==1) & (lr[0]==1):  # left choice & left reward, where lr[0] is the lreward_available\n",
    "            trial_history_dict['Reward'][-1] = 1\n",
    "        elif (choice==2) & (lr[1]==1):\n",
    "            trial_history_dict['Reward'][-1] = 1\n",
    "        else:\n",
    "            trial_history_dict['Reward'][-1] = 0  # This should be redundant, but just to be safe\n",
    "\n",
    "    trial_history_pd = pd.DataFrame(trial_history_dict)\n",
    "    return trial_history_pd\n",
    "\n",
    "# Environment Simulation\n",
    "def simulation_reward_assignment(trial_history_dict, current_trial, environment_properties):\n",
    "    # Define fixed parameters\n",
    "    betaRC = environment_properties['betaRC']\n",
    "    betaUC = environment_properties['betaUC']\n",
    "    nback = environment_properties['nback']\n",
    "    tau = environment_properties['tau']\n",
    "    Pmax = environment_properties['Pmax']\n",
    "    \n",
    "    # Create exponential filter inside this function\n",
    "    n = np.array(range(1, Num_Trials))\n",
    "    exponential_filter = np.exp((1-n)/tau)\n",
    "\n",
    "    if current_trial == 1:\n",
    "        nomissalarm = 1\n",
    "        lreward_available = 0\n",
    "        rreward_available = 0\n",
    "        l_prob = 0.5\n",
    "        r_prob = 0.5\n",
    "    else:\n",
    "        a = np.array(trial_history_dict['Choice'])\n",
    "        a_cleaned = a.copy()+0.\n",
    "        a_cleaned[a>2] = 0\n",
    "        a_cleaned[a==2] = -1\n",
    "        R = np.array(trial_history_dict['Reward'])\n",
    "        UR = 1-R\n",
    "        RewC = a_cleaned*R\n",
    "        UnrC = a_cleaned*UR\n",
    "\n",
    "        if current_trial < nback+1:\n",
    "            RewC_cleaned = RewC[::-1][0:current_trial-1]\n",
    "            UnrC_cleaned = UnrC[::-1][0:current_trial-1]\n",
    "            RC_weights = RewC_cleaned*exponential_filter[0:current_trial-1]\n",
    "            UC_weights = UnrC_cleaned*(exponential_filter[0:current_trial-1])\n",
    "        else:\n",
    "            RewC_cleaned = RewC[::-1][0:nback]\n",
    "            UnrC_cleaned = UnrC[::-1][0:nback]\n",
    "            RC_weights = RewC_cleaned*exponential_filter[0:nback]\n",
    "            UC_weights = UnrC_cleaned*(exponential_filter[0:nback])\n",
    "        \n",
    "        sum_weights = betaRC*np.sum(RC_weights) + betaUC*sum(UC_weights)\n",
    "        l_prob = Pmax/(1+np.exp(-sum_weights))\n",
    "        r_prob = Pmax-l_prob\n",
    "\n",
    "        # Confirm a choice was made on the previous trial\n",
    "        nomissalarm = (trial_history_dict['Choice'][-1]==1) | (trial_history_dict['Choice'][-1]==2)\n",
    "        # Load reward baiting\n",
    "        lreward_available = trial_history_dict['lreward_available'][-1]\n",
    "        rreward_available = trial_history_dict['rreward_available'][-1]\n",
    "\n",
    "        # If the previous choice was rewarded, clear reward\n",
    "        if (trial_history_dict['Reward'][-1]==1) & (trial_history_dict['Choice'][-1]==1):\n",
    "            lreward_available = 0\n",
    "        elif (trial_history_dict['Reward'][-1]==1) & (trial_history_dict['Choice'][-1]==2):\n",
    "            rreward_available = 0\n",
    "\n",
    "    # Assign reward\n",
    "    if nomissalarm == 1:\n",
    "        rreward_available = (rreward_available==1) | (random.random() < r_prob)\n",
    "        lreward_available = (lreward_available==1) | (random.random() < l_prob)\n",
    "\n",
    "    rreward_available += 0\n",
    "    lreward_available += 0\n",
    "\n",
    "    # Update and append to trial_history_dict\n",
    "    trial_history_dict = {'Trial': np.append(trial_history_dict['Trial'], current_trial),\n",
    "                         'l_prob': np.append(trial_history_dict['l_prob'], l_prob),\n",
    "                         'r_prob': np.append(trial_history_dict['r_prob'], r_prob),\n",
    "                         'lreward_available': np.append(trial_history_dict['lreward_available'], lreward_available),\n",
    "                         'rreward_available': np.append(trial_history_dict['rreward_available'], rreward_available),\n",
    "                         'Choice': np.append(trial_history_dict['Choice'], 0),\n",
    "                         'Reward': np.append(trial_history_dict['Reward'], 0),\n",
    "                         'p_left': np.append(trial_history_dict['p_left'], 0.5)}  # is a place holder\n",
    "\n",
    "    lr_reward_availibility = [lreward_available, rreward_available]\n",
    "    return trial_history_dict, lr_reward_availibility\n",
    "\n",
    "# Agent simulation\n",
    "def simulation_agent(trial_history_dict, current_trial, agent_properties):\n",
    "    # Simulation agent decision section\n",
    "    if agent_properties['type'] == 'random':\n",
    "        choice = random.randint(1, 2)\n",
    "        p_lr = np.array([0.5, 0.5])\n",
    "        if random.random() < p_lr[0]:\n",
    "            choice = 1  # left choice\n",
    "        else:\n",
    "            choice = 2  # right choice\n",
    "\n",
    "    if agent_properties['type'] == 'WinStay_LosesWitch':\n",
    "        if current_trial == 1:\n",
    "            p_lr = np.array([0.5, 0.5])\n",
    "        else:\n",
    "            a_previous = trial_history_dict['Choice'][current_trial-2]\n",
    "            R_previous = trial_history_dict['Reward'][current_trial-2]\n",
    "            if a_previous == 1:  # left choice trials\n",
    "                if R_previous == 1:\n",
    "                    p_lr = np.array([1, 0])\n",
    "                else:\n",
    "                    p_lr = np.array([0, 1])\n",
    "            elif a_previous == 2:  # right choice trials\n",
    "                if R_previous == 1:\n",
    "                    p_lr = np.array([0, 1])\n",
    "                else:\n",
    "                    p_lr = np.array([1, 0])\n",
    "            else:  # miss/alarm, stay\n",
    "                print('include miss/alarm')\n",
    "\n",
    "        if agent_properties['strategy'] == 'deterministic':\n",
    "            if p_lr[0] >= 0.5:\n",
    "                choice = 1  # left choice\n",
    "            else:\n",
    "                choice = 2  # right choice\n",
    "        else:  # probabilistic\n",
    "            if random.random() < p_lr[0]:\n",
    "                choice = 1  # left choice\n",
    "            else:\n",
    "                choice = 2  # right choice\n",
    "\n",
    "    if agent_properties['type'] == 'Foraging1B_GT':\n",
    "        nback = 1\n",
    "        agent_tau = 0.01\n",
    "        agent_n = np.array(range(1, Num_Trials))\n",
    "        agent_exponential_filter = np.exp((1-agent_n)/agent_tau)\n",
    "        if current_trial == 1:\n",
    "            p_lr = np.array([0.5, 0.5])\n",
    "        else:\n",
    "            a = np.array(trial_history_dict['Choice'][0:current_trial-1])\n",
    "            a_cleaned = a.copy()+0.\n",
    "            a_cleaned[a>2] = 0\n",
    "            a_cleaned[a==2] = -1\n",
    "            R = np.array(trial_history_dict['Reward'][0:current_trial-1])\n",
    "            UR = 1-R\n",
    "            RewC = a_cleaned*R\n",
    "            UnrC = a_cleaned*UR\n",
    "\n",
    "            if current_trial < nback+1:\n",
    "                RewC_cleaned = RewC[::-1][0:current_trial-1]\n",
    "                UnrC_cleaned = UnrC[::-1][0:current_trial-1]\n",
    "                RC_weights = RewC_cleaned*agent_exponential_filter[0:current_trial-1]\n",
    "                UC_weights = UnrC_cleaned*(agent_exponential_filter[0:current_trial-1])\n",
    "            else:\n",
    "                RewC_cleaned = RewC[::-1][0:nback]\n",
    "                UnrC_cleaned = UnrC[::-1][0:nback]\n",
    "                RC_weights = RewC_cleaned*agent_exponential_filter[0:nback]\n",
    "                UC_weights = UnrC_cleaned*(agent_exponential_filter[0:nback])\n",
    "            \n",
    "            sum_weights = 1.5*np.sum(RC_weights) + (-6)*sum(UC_weights)\n",
    "            l_prob = 1/(1+np.exp(-sum_weights))\n",
    "            p_lr = np.array([l_prob, 1-l_prob])\n",
    "            \n",
    "        if agent_properties['strategy'] == 'deterministic':\n",
    "            if p_lr[0] >= 0.5:\n",
    "                choice = 1  # left choice\n",
    "            else:\n",
    "                choice = 2  # right choice\n",
    "        else:  # probabilistic\n",
    "            if random.random() < p_lr[0]:\n",
    "                choice = 1  # left choice\n",
    "            else:\n",
    "                choice = 2  # right choice\n",
    "\n",
    "    if agent_properties['type'] == 'Foraging20B_GT':\n",
    "        nback = 20\n",
    "        agent_tau = 20\n",
    "        agent_n = np.array(range(1, Num_Trials))\n",
    "        agent_exponential_filter = np.exp((1-agent_n)/agent_tau)\n",
    "        if current_trial == 1:\n",
    "            p_lr = np.array([0.5, 0.5])\n",
    "        else:\n",
    "            a = np.array(trial_history_dict['Choice'][0:current_trial-1])\n",
    "            a_cleaned = a.copy()+0.\n",
    "            a_cleaned[a>2] = 0\n",
    "            a_cleaned[a==2] = -1\n",
    "            R = np.array(trial_history_dict['Reward'][0:current_trial-1])\n",
    "            UR = 1-R\n",
    "            RewC = a_cleaned*R\n",
    "            UnrC = a_cleaned*UR\n",
    "\n",
    "            if current_trial < nback+1:\n",
    "                RewC_cleaned = RewC[::-1][0:current_trial-1]\n",
    "                UnrC_cleaned = UnrC[::-1][0:current_trial-1]\n",
    "                RC_weights = RewC_cleaned*agent_exponential_filter[0:current_trial-1]\n",
    "                UC_weights = UnrC_cleaned*(agent_exponential_filter[0:current_trial-1])\n",
    "            else:\n",
    "                RewC_cleaned = RewC[::-1][0:nback]\n",
    "                UnrC_cleaned = UnrC[::-1][0:nback]\n",
    "                RC_weights = RewC_cleaned*agent_exponential_filter[0:nback]\n",
    "                UC_weights = UnrC_cleaned*(agent_exponential_filter[0:nback])\n",
    "            \n",
    "            sum_weights = 0.5*np.sum(RC_weights) + (-1)*sum(UC_weights)\n",
    "            l_prob = 1/(1+np.exp(-sum_weights))\n",
    "            p_lr = np.array([l_prob, 1-l_prob])\n",
    "            \n",
    "        if agent_properties['strategy'] == 'deterministic':\n",
    "            if p_lr[0] >= 0.5:\n",
    "                choice = 1  # left choice\n",
    "            else:\n",
    "                choice = 2  # right choice\n",
    "        else:  # probabilistic\n",
    "            if random.random() < p_lr[0]:\n",
    "                choice = 1  # left choice\n",
    "            else:\n",
    "                choice = 2  # right choice\n",
    "\n",
    "    if agent_properties['type'] == 'bias':\n",
    "        const_bias = -0.7  # beta0, bias term in the RL model\n",
    "        r = 2*random.random()-1\n",
    "        if r+const_bias > 0:\n",
    "            choice = 1\n",
    "        else:\n",
    "            choice = 2\n",
    "        p_lr = np.array([0.5, 0.5])\n",
    "\n",
    "    # Update trial history dictionary with decision parameters\n",
    "    trial_history_dict['p_left'][-1] = p_lr[0]\n",
    "\n",
    "    return trial_history_dict, choice\n",
    "\n",
    "# Configuration\n",
    "Environments = ['Long', 'Short']  # Run both environments\n",
    "Simulation_Agent = 'Foraging1B_GT'  # Options: \"WinStay_LosesWitch\", \"bias\", \"random\", \"Foraging1B_GT\", \"Foraging20B_GT\"\n",
    "Decision_Strategy = 'probabilistic'  # Options: \"deterministic\", \"probabilistic\"\n",
    "Num_Trials = 500\n",
    "Miss_Rate = 0\n",
    "Alarm_Rate = 0\n",
    "Num_Runs = 100  # Number of simulation runs for each agent type\n",
    "\n",
    "# Set up environment properties\n",
    "environment_properties_dict = {\n",
    "    'Long': {'betaRC': 0.5,\n",
    "             'betaUC': -1,\n",
    "             'nback': 20,\n",
    "             'tau': 20,\n",
    "             'Pmax': 0.65},\n",
    "    'Short': {'betaRC': 1.5,\n",
    "              'betaUC': -6,\n",
    "              'nback': 1,\n",
    "              'tau': 0.01,\n",
    "              'Pmax': 0.75}\n",
    "}\n",
    "\n",
    "# Function to run simulations for a given environment\n",
    "def run_simulations_for_environment(env_name):\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"Starting simulations for {env_name} environment\")\n",
    "    print(f\"{'='*50}\")\n",
    "    \n",
    "    # Set up environment\n",
    "    environment_properties = environment_properties_dict[env_name]\n",
    "    n = np.array(range(1, Num_Trials))\n",
    "    tau = environment_properties['tau']\n",
    "    hyperbolic_filter = 1./(1+(n-1)/tau)\n",
    "    exponential_filter = np.exp((1-n)/tau)\n",
    "    n_back = environment_properties['nback']\n",
    "    \n",
    "    # Initialize dataframe for storing all simulations\n",
    "    simulation_df = pd.DataFrame(columns=['Environment', 'Agent', 'a', 'R', 'L(prob)'])\n",
    "    \n",
    "    # Run simulations for Foraging1B_GT\n",
    "    Simulation_Agent = 'Foraging1B_GT'\n",
    "    agent_properties = {'type': Simulation_Agent,\n",
    "                       'miss rate': Miss_Rate,\n",
    "                       'alarm rate': Alarm_Rate,\n",
    "                       'strategy': Decision_Strategy}\n",
    "    \n",
    "    print(f\"\\nRunning {Num_Runs} simulations for {Simulation_Agent}\")\n",
    "    for i in range(Num_Runs):\n",
    "        trial_history = simulation_session_loop(environment_properties, agent_properties, Num_Trials)\n",
    "        a = np.array(trial_history['Choice'])\n",
    "        R = np.array(trial_history['Reward'])\n",
    "        l_prob = trial_history['l_prob']\n",
    "        \n",
    "        temp_dict = {'Environment': env_name,\n",
    "                    'Agent': Simulation_Agent,\n",
    "                    'a': a, \n",
    "                    'R': R, \n",
    "                    'L(prob)': l_prob}\n",
    "        \n",
    "        simulation_df = pd.concat([simulation_df, pd.DataFrame([temp_dict])], ignore_index=True)\n",
    "    \n",
    "    # Run simulations for Foraging20B_GT\n",
    "    Simulation_Agent = 'Foraging20B_GT'\n",
    "    agent_properties = {'type': Simulation_Agent,\n",
    "                       'miss rate': Miss_Rate,\n",
    "                       'alarm rate': Alarm_Rate,\n",
    "                       'strategy': Decision_Strategy}\n",
    "    \n",
    "    print(f\"\\nRunning {Num_Runs} simulations for {Simulation_Agent}\")\n",
    "    for i in range(Num_Runs):\n",
    "        trial_history = simulation_session_loop(environment_properties, agent_properties, Num_Trials)\n",
    "        a = np.array(trial_history['Choice'])\n",
    "        R = np.array(trial_history['Reward'])\n",
    "        l_prob = trial_history['l_prob']\n",
    "        \n",
    "        temp_dict = {'Environment': env_name,\n",
    "                    'Agent': Simulation_Agent,\n",
    "                    'a': a, \n",
    "                    'R': R, \n",
    "                    'L(prob)': l_prob}\n",
    "        \n",
    "        simulation_df = pd.concat([simulation_df, pd.DataFrame([temp_dict])], ignore_index=True)\n",
    "    \n",
    "    # Save the dataframe\n",
    "#     save_directory = 'set your save directory here'\n",
    "    os.makedirs(save_directory, exist_ok=True)\n",
    "    \n",
    "    output_filename = os.path.join(save_directory, f'simulation_data_{env_name}_combined_{random_seed}.pkl')\n",
    "    pickle.dump(simulation_df, open(output_filename, 'wb'))\n",
    "    print(f\"\\nSimulation data saved to {output_filename}\")\n",
    "    \n",
    "    \n",
    "    n_back = 20\n",
    "    # Calculate coefficients\n",
    "    print(\"Calculating regression coefficients...\")\n",
    "    coef_df_green = pd.DataFrame(columns=range(n_back*2))\n",
    "    coef_df_yellow = pd.DataFrame(columns=range(n_back*2))\n",
    "    intercept_all_green = []\n",
    "    intercept_all_yellow = []\n",
    "    \n",
    "    for ss in range(len(simulation_df)):\n",
    "        a = simulation_df.iloc[ss]['a']\n",
    "        R = simulation_df.iloc[ss]['R']\n",
    "        glm_mat, glm_target = logistic_prepare_predictors(a, R, 20)\n",
    "        \n",
    "        log_reg = LogisticRegression(solver='lbfgs', penalty='none',\n",
    "                                    n_jobs=-1, random_state=0, multi_class='auto', \n",
    "                                    fit_intercept=True, max_iter=10000)\n",
    "        \n",
    "        log_reg.fit(glm_mat, glm_target)\n",
    "        coef = log_reg.coef_[0]\n",
    "        intercept = log_reg.intercept_[0]\n",
    "        \n",
    "        if simulation_df['Agent'][ss] == \"Foraging1B_GT\":\n",
    "            coef_df_green.loc[ss, :] = coef\n",
    "            intercept_all_green.append(intercept)\n",
    "        elif simulation_df['Agent'][ss] == \"Foraging20B_GT\":\n",
    "            coef_df_yellow.loc[ss, :] = coef\n",
    "            intercept_all_yellow.append(intercept)\n",
    "    \n",
    "    # Save coefficients\n",
    "    coef_filename = os.path.join(save_directory, f'coefficients_{env_name}_combined_{random_seed}.pkl')\n",
    "    pickle.dump({'coef_df_green': coef_df_green, 'coef_df_yellow': coef_df_yellow, \n",
    "                'intercept_all_green': intercept_all_green, 'intercept_all_yellow': intercept_all_yellow}, \n",
    "                open(coef_filename, 'wb'))\n",
    "    print(f\"Regression coefficients calculated and saved to {coef_filename}\")\n",
    "    \n",
    "    # Plotting\n",
    "    print(\"Generating plots...\")\n",
    "    common_x = np.arange(-n_back, 0, 1)\n",
    "    fig, ax = plt.subplots(1, 2, figsize=(8, 4), sharey=False, sharex=True, tight_layout=True)\n",
    "    \n",
    "    # RewC\n",
    "    ax[0].errorbar(common_x, coef_df_green.mean(axis=0)[:n_back],\n",
    "                   yerr=[sem(coef_df_green.iloc[:, w].dropna()) for w in range(n_back)],\n",
    "                   color='green', marker='o', markersize=3, linestyle='-', \n",
    "                   lw=2, capsize=4, label='Foraging1B_GT')\n",
    "    ax[0].errorbar(common_x, coef_df_yellow.mean(axis=0)[:n_back],\n",
    "                   yerr=[sem(coef_df_yellow.iloc[:, w].dropna()) for w in range(n_back)],\n",
    "                   color='orange', marker='o', markersize=3, linestyle='-', \n",
    "                   lw=2, capsize=4, label='Foraging20B_GT')\n",
    "    \n",
    "    # UnrC\n",
    "    ax[1].errorbar(common_x, coef_df_green.mean(axis=0)[n_back:2*n_back],\n",
    "                   yerr=[sem(coef_df_green.iloc[:, w + n_back].dropna()) for w in range(n_back)],\n",
    "                   color='green', marker='o', markersize=3, linestyle='-', \n",
    "                   lw=2, capsize=4, label='Foraging1B_GT')\n",
    "    ax[1].errorbar(common_x, coef_df_yellow.mean(axis=0)[n_back:2*n_back],\n",
    "                   yerr=[sem(coef_df_yellow.iloc[:, w + n_back].dropna()) for w in range(n_back)],\n",
    "                   color='orange', marker='o', markersize=3, linestyle='-', \n",
    "                   lw=2, capsize=4, label='Foraging20B_GT')\n",
    "    \n",
    "    # Formatting\n",
    "    for i in [0, 1]:\n",
    "        ax[i].axhline(y=0, color='k', linestyle=':')\n",
    "        ax[i].legend(frameon=False)\n",
    "        ax[i].spines['top'].set_visible(False)\n",
    "        ax[i].spines['right'].set_visible(False)\n",
    "    \n",
    "    ax[0].set_ylim([-2, 6])\n",
    "    ax[0].set_xlabel(\"Past Trials\", fontsize=13)\n",
    "    ax[0].set_ylabel(r\"$\\beta_{RewC(t-i)}$\", fontsize=15)\n",
    "    ax[0].set_title(\"RewC\")\n",
    "    ax[1].set_ylim([-25, 2])\n",
    "    ax[1].set_xlabel(\"Past Trials\", fontsize=13)\n",
    "    ax[1].set_ylabel(r\"$\\beta_{UnrC(t-i)}$\", fontsize=15)\n",
    "    ax[1].set_title(\"UnrC\")\n",
    "    \n",
    "    # Save plots\n",
    "    plot_filename_png = os.path.join(save_directory, f'weights_plot_{env_name}_combined_{random_seed}.png')\n",
    "    plot_filename_svg = os.path.join(save_directory, f'weights_plot_{env_name}_combined_{random_seed}.svg')\n",
    "    \n",
    "    plt.savefig(plot_filename_png, dpi=300, bbox_inches='tight')\n",
    "    plt.savefig(plot_filename_svg, format='svg', bbox_inches='tight')\n",
    "    print(f\"Plots saved to:\")\n",
    "    print(f\"PNG: {plot_filename_png}\")\n",
    "    print(f\"SVG: {plot_filename_svg}\")\n",
    "    \n",
    "    plt.close()  # Close the figure to free memory\n",
    "\n",
    "# Run simulations for both environments\n",
    "for env in Environments:\n",
    "    run_simulations_for_environment(env)\n",
    "\n",
    "print(\"\\nAll simulations completed.\") "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
